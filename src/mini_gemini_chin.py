import os
import jieba
from typing import List

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

if "GOOGLE_API_KEY" not in os.environ:
    raise RuntimeError("Please set the GOOGLE_API_KEY in your environment variables first.")

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004"
)

CHROMA_PATH = "./chroma_db_chin"
if not os.path.exists(CHROMA_PATH):
    raise RuntimeError(f"âŒ Vector DB not found at {CHROMA_PATH}! Please run your ingest script first.")

vector_store = Chroma(
    persist_directory=CHROMA_PATH,
    embedding_function=embeddings,
    collection_name="demo_rag"
)

print("æ­£åœ¨åˆå§‹åŒ–æ··åˆæª¢ç´¢ç³»çµ± (Vector + BM25 with Jieba)...")

existing_data = vector_store.get() 
existing_texts = existing_data['documents']
existing_metadatas = existing_data['metadatas']

if not existing_texts:
    raise RuntimeError("Chroma DB is empty! Cannot initialize BM25.")

doc_objects = [
    Document(page_content=text, metadata=meta) 
    for text, meta in zip(existing_texts, existing_metadatas)
]

# å®šç¾©ä¸­æ–‡æ–·è©å‡½æ•¸
def chinese_tokenizer(text: str) -> List[str]:
    """
    ä½¿ç”¨ jieba é€²è¡Œä¸­æ–‡æ–·è©ã€‚
    BM25 éœ€è¦ list of tokensï¼Œè€Œä¸æ˜¯ raw stringã€‚
    """
    return jieba.lcut(text)

# å»ºç«‹ BM25 Retriever (é—œéµå­—æœå°‹)
# å‚³å…¥ preprocess_func=chinese_tokenizer
bm25_retriever = BM25Retriever.from_documents(
    documents=doc_objects,
    preprocess_func=chinese_tokenizer 
)
bm25_retriever.k = 5

chroma_retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, chroma_retriever],
    weights=[0.5, 0.5] 
)

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", 
    temperature=0.3, 
)

system_prompt = (
    "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„ã€ä¸Šä¸‹æ–‡è³‡è¨Šã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚"
    "è«‹å‹™å¿…ä½¿ç”¨**ç¹é«”ä¸­æ–‡ (Traditional Chinese)** å›ç­”ã€‚"
    "å¦‚æœä½ åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªä½ ä¸çŸ¥é“ï¼Œä¸è¦ç·¨é€ ç­”æ¡ˆã€‚"
    "\n\n"
    "ã€ä¸Šä¸‹æ–‡è³‡è¨Šã€‘:\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(ensemble_retriever, question_answer_chain)

if __name__ == "__main__":
    print("\n=========================================")
    print("ğŸ¤– Gemini RAG System Ready (Hybrid Search + Jieba)")
    print("=========================================\n")

    while True:
        try:
            query = input("è«‹è¼¸å…¥å•é¡Œ (è¼¸å…¥ 'exit' é›¢é–‹) > ")
            if query.lower() in ["exit", "quit"]:
                break
            
            if not query.strip():
                continue

            print("\næ­£åœ¨æ€è€ƒä¸­...\n")
            
            # åŸ·è¡Œ Chain
            response = rag_chain.invoke({"input": query})

            print(f"A: {response['answer']}")
            print("-" * 60)

        except Exception as e:
            print(f"Error: {e}")